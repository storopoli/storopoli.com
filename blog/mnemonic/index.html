<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com https://www.youtube.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://storopoli.com name=base><title>
Jose Storopoli, PhD • Seed phrases and entropy</title><link href=https://storopoli.com/favicon.svg rel=icon type=image/png><link title="Jose Storopoli, PhD - Atom Feed" href=https://storopoli.com/atom.xml rel=alternate type=application/atom+xml><link href="https://storopoli.com/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://storopoli.com/main.css?h=6b1dda977391dad0c76e" rel=stylesheet><link href="https://storopoli.com/extra.css?h=33c2dfb527f758787862" rel=stylesheet><link href="https://storopoli.com/skins/teal.css?h=bd19e558a52d678a50de" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Personal website of Jose Storopoli, PhD" name=description><meta content="Personal website of Jose Storopoli, PhD" property=og:description><meta content="Seed phrases and entropy" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://storopoli.com/blog/mnemonic/ property=og:url><meta content="Jose Storopoli, PhD" property=og:site_name><noscript><link href=https://storopoli.com/no_js.css rel=stylesheet></noscript><script src=https://storopoli.com/js/initializeTheme.min.js></script><script defer src=https://storopoli.com/js/themeSwitcher.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://storopoli.com>Jose Storopoli, PhD</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://storopoli.com/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://storopoli.com/tags/>tags </a><li class=menu-icons-container><ul class=menu-icons-group><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article><h1 class=article-title>Seed phrases and entropy</h1><ul class=meta><span class="hidden p-author h-card"> <a title="Jose Storopoli, PhD" class=u-url href=https://storopoli.com rel=author>Jose Storopoli, PhD</a> </span><li>11th Feb 2024<li title="2068 words"><span aria-hidden=true class=separator>•</span>11 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a href=https://storopoli.com/tags/cryptography/>cryptography</a>, <li class=tag><a href=https://storopoli.com/tags/probability/>probability</a>, <li class=tag><a href=https://storopoli.com/tags/bitcoin/>bitcoin</a></ul><div class=toc-container><h3>Table of Contents</h3><ul><li><a href=https://storopoli.com/blog/mnemonic/#entropy>Entropy</a> <ul><li><a href=https://storopoli.com/blog/mnemonic/#how-the-hell-physics-came-to-passwords>How the hell Physics came to Passwords?</a><li><a href=https://storopoli.com/blog/mnemonic/#entropy-and-passwords>Entropy and Passwords</a></ul><li><a href=https://storopoli.com/blog/mnemonic/#bitcoin-seed-phrases>Bitcoin Seed Phrases</a><li><a href=https://storopoli.com/blog/mnemonic/#seed-phrases-and-passwords>Seed Phrases and Passwords</a><li><a href=https://storopoli.com/blog/mnemonic/#conclusion>Conclusion</a></ul></div><section class=body><noscript><div class="admonition warning"><div class="admonition-icon admonition-icon-warning"></div><div class=admonition-content><strong class=admonition-title> Evil JavaScript </strong><p>This post uses <a href=https://katex.org/>KaTeX</a> to render mathematical expressions.<p>To see the rendered mathematical expressions, you’ll need to enable JavaScript.</div></div></noscript><p><img alt="Password meme" src=https://storopoli.com/blog/mnemonic/password_strength.png><p>In this post, let’s dive into a topic that is very important for anyone who uses the internet: <strong>passwords</strong>. We’ll cover what the hell is <strong>Entropy</strong>, good <strong>password practices</strong>, and how it relates to <strong>Bitcoin “seed phrases”</strong>.<blockquote><p><a href="https://storopoli.com/blog/mnemonic/$block.attrs('info')"></a> seed phrases are technically called “mnemonic phrases”, but I’ll use the term “seed phrases” for the rest of the post.</blockquote><h2 id=entropy>Entropy</h2><p>Before we go into passwords, I’ll introduce the concept of <strong><em>Entropy</em></strong>.<p><a href=https://en.wikipedia.org/wiki/Entropy>Entropy</a> is a measure of the <strong>amount of disorder in a system</strong>. It has its origins in <strong>Thermodynamics</strong>, where it’s used to measure the amount of energy in a system that is not available to do work.<p>The etymology of the word “Entropy” is after the Greek word for “transformation”.<p>It was given a proper statistical definition by <a href=https://en.wikipedia.org/wiki/Ludwig_Boltzmann>Ludwig Boltzmann</a> in 1870s. while establishing the field of <a href=https://en.wikipedia.org/wiki/Statistical_dynamics>Statistical Dynamics</a>, a field of physics that studies the behavior of large collections of particles.<p><img alt="Ludwig Boltzmann" src=https://storopoli.com/blog/mnemonic/boltzmann.jpg><p>In the context of Statistical Dynamics, <strong>Entropy is a measure of the number of ways a system can be arranged</strong>. The more ways a system can be arranged, the higher its Entropy. Specifically, <strong>Entropy is a logarithmic measure of the number of system states with significant probability of being occupied</strong>:<p>$$S = -k \cdot \sum_i p_i \ln p_i$$<p>Where:<ul><li>$S$: Entropy.<li>$k$: Boltzmann’s constant, a physical constant that relates temperature to energy.<li>$p_i$: probability of the system being in state $i$.</ul><p>In this formula, if all states are equally likely, i.e $p_i = \frac{1}{N}$, where $N$ is the number of states, then the entropy is maximized. You can see this since a probability $p$ is a real number between 0 and 1, and as $N$ approaches infinity, the sum of the logarithms approaches negative infinity. Then, multiplying by $-k$ yields positive infinity.<h3 id=how-the-hell-physics-came-to-passwords>How the hell Physics came to Passwords?</h3><p>There’s once a great men called <a href=https://en.wikipedia.org/wiki/Claude_Shannon>Claude Shannon</a>, who single-handedly founded the field of <a href=https://en.wikipedia.org/wiki/Information_theory><strong>Information Theory</strong></a>, invented the concept of a <a href=https://en.wikipedia.org/wiki/Bit><strong>Bit</strong></a>, and was the first to think about Boolean algebra in the context of electrical circuits. He laid the foundation for the <a href=https://en.wikipedia.org/wiki/Digital_Revolution><strong>Digital Revolution</strong></a>.<p>If you are happy using your smartphone, laptop, or any other digital device, in you high speed fiber internet connection, through a wireless router to send cats pictures to your friends, then you should thank Claude Shannon.<p><img alt="Claude Shannon" src=https://storopoli.com/blog/mnemonic/shannon.jpg><p>He was trying to find a formula to quantify the amount of information in a message. He wanted three things:<ol><li>The measure should be a <strong>function of the probability of the message</strong>. Messages that are more likely should have less information.<li>The measure should be <strong>additive</strong>. The information in a message should be the sum of the information in its parts.<li>The measure should be <strong>continuous</strong>. Small changes in the message should result in small changes in the measure.</ol><p>He pretty much found that the formula for Entropy in statistical mechanics was a good measure of information. He called it <em>Entropy</em> to honor Boltzmann’s work. To differentiate it from the Statistical Dynamics’ Entropy, he changed the letter to $H$, in honor of <a href=https://en.wikipedia.org/wiki/H-theorem>Boltzmann’s $H$-theorem</a>. So the formula for the Entropy of a message is:<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​)$$<p>Where:<ul><li>$X$: random discrete variable.<li>$H(X)$: Entropy of $X$<li>$P(x_i)$: probability of the random variable $X$ taking the value $x_i$. Also known as the probability mass function (PMF) of the discrete random variable $X$.<li>$\log$: base 2 logarithm, to measure the Entropy in bits.</ul><p>In information theory, the <strong>Entropy of a random variable is the average level of “information”, “surprise”, or “uncertainty” inherent to the variable’s possible outcomes</strong>.<p>Let’s take the simple example of a fair coin. The Entropy of the random variable $X$ that represents the outcome of a fair coin flip is:<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{2}\right) = 1 \text{ bit}$$<p>So the outcome of a fair coin flip has 1 bit of Entropy. This means that the outcome of a fair coin flip has 1 bit of information, or 1 bit of uncertainty. Once the message is received, that the coin flip was heads or tails, the receiver has 1 bit of information about the outcome.<p>Alternatively, we only need 1 bit to encode the outcome of a fair coin flip. Hence, there’s a connection between Entropy, search space, and information.<p>Another good example is the outcome of a fair 6-sided die. The Entropy of the random variable $X$ that represents the outcome of a fair 6-sided die is:<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = - \sum_{i=1}^6\left(\frac{1}{6} * \log \frac{1}{6} \right) \approx 2.58 \text{ bits}$$<p>This means that the outcome of a fair 6-sided die has 2.58 bits of Entropy. we need $\operatorname{ceil}(2.58) = 3$ bits to encode the outcome of a fair 6-sided die.<h3 id=entropy-and-passwords>Entropy and Passwords</h3><p>Ok now we come full circle. Let’s talk, finally, about passwords.<p>In the context of passwords, <strong>Entropy</strong> is a measure of how unpredictable a password is. The higher the Entropy, the harder it is to guess the password. The Entropy of a password is measured in bits, and it’s calculated using the formula:<p>$$H = L \cdot \log_2(N)$$<p>Where:<ul><li>$H$: Entropy in bits<li>$N$: number of possible characters in the password<li>$L$: length of the password<li>$\log_2$:​ (N) calculates how many bits are needed to represent each character from the set.</ul><p>For example, if we have a password with 8 characters and each character can be any of the 26 lowercase letters, the standard english alphabet, the Entropy would be:<p>$$H = 8 \cdot \log_2(26) \approx 37.6 \text{ bits}$$<p>This means that an attacker would need to try $2^{37.6} \approx 2.01 \cdot 10^{11}$ combinations to guess the password.<blockquote><p><a href="https://storopoli.com/blog/mnemonic/$block.attrs('info')"></a> Technically, we need to divide the number of combinations by 2, since we are assuming that the attacker is using a brute-force attack, which means that the attacker is trying all possible combinations, and the password could be at the beginning or at the end of the search space. This is called the <a href=https://en.wikipedia.org/wiki/Birthday_problem>birthday paradox</a>, and it assumes that the password is uniformly distributed in the search space.</blockquote><p>If the password were to include uppercase letters, numbers, and symbols (let’s assume 95 possible characters in total), the Entropy for an 8-character password would be:<p>$$H = 8 \cdot \log_2(95) \approx 52.6 \text{ bits}$$<p>This means that an attacker would need to try $2^{52.6} \approx 6.8 \cdot 10^{15}$ combinations to guess the password.<p>This sounds a lot but it’s not that much.<p>For the calculations below, we’ll assume that the attacker now your dictionary set, i.e. the set of characters you use to create your password, and the password length.<p>If an attacker get a hold of an NVIDIA RTX 4090, MSRP USD 1,599, which can do <a href=https://www.tomshardware.com/news/rtx-4090-password-cracking-comparison>300 GH/s (300,000,000,000 hashes/second)</a>, i.e. $3 \cdot 10^{11}$ hashes/second, it would take:<ol><li>8-length lowercase-only password:</ol><p>$$\frac{2.01 \cdot 10^{11}}{3 \cdot 10^{11}} \approx 0.67 \text{ seconds}$$<ol><li>8-length password with uppercase letters, numbers, and symbols:</ol><p>$$\frac{6.8 \cdot 10^{15}}{3 \cdot 10^{11}} \approx 22114 \text{ seconds} \approx 6.14 \text{ hours}$$<p>So, the first password would be cracked in less than a second, while the second would take a few hours. This with just one 1.5k USD GPU.<h2 id=bitcoin-seed-phrases>Bitcoin Seed Phrases</h2><p>Now that we understand Entropy and how it relates to passwords, let’s talk about bitcoin seed phrases.<p>Remember that our private key is a big-fucking number? If not, check my <a href="https://storopoli.com/blog/mnemonic/$link.page('blog/2024-02-05-crypto-basics')">post on cryptographics basics</a>.<p><a href=https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki>BIP-39</a> specifies how to use easy-to-remember seed phrases to store and recover private keys. The <a href=https://github.com/bitcoin/bips/blob/master/bip-0039/english.txt>wordlist</a> adheres to the following principles:<ol><li><strong>smart selection of words</strong>: the wordlist is created in such a way that it’s enough to type the first four letters to unambiguously identify the word.<li><strong>similar words avoided</strong>: word pairs like “build” and “built”, “woman” and “women”, or “quick” and “quickly” not only make remembering the sentence difficult but are also more error prone and more difficult to guess.</ol><p>Here is a simple 7-word seed phrase: <code>brave sadness grocery churn wet mammal tube</code>. Surprisingly enough, this badboy here gives you $77$ bits of Entropy, while also being easy to remember. This is due to the fact that the wordlist has 2048 words, so each word gives you $\log_2(2048) = 11$ bits of Entropy.<p>There’s a minor caveat to cover here. The last word in the seed phrase is a checksum, which is used to verify that the phrase is valid.<p>So, if you have a 12-word seed phrase, you have $11 \cdot 11 = 121$ bits of Entropy. And for a 24-word seed phrase, you have $23 \cdot 11 = 253$ bits of Entropy.<p>The National Institute of Standards and Technology (NIST) recommends a <a href=https://crypto.stackexchange.com/a/87059>minimum of 112 bits of Entropy for all things cryptographic</a>. And Bitcoin has a <a href=https://bitcoin.stackexchange.com/a/118929>minimum of 128 bits of Entropy</a>.<p>Depending on your threat model, <a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>“Assume that your adversary is capable of a trillion guesses per second”</a>, it can take a few years to crack a 121-bit Entropy seed phrase:<p>$$\frac{2^{121}}{10^{12}} \approx 2.66 \cdot 10^{24} \text{ seconds} \approx 3.08 \cdot 10^{19} \text{ days} \approx 8.43 \cdot 10^{16} \text{ years}$$<p>That’s a lot of years. Now for a 253-bit Entropy seed phrase:<p>$$\frac{2^{253}}{10^{12}} \approx 1.45 \cdot 10^{64} \text{ seconds} \approx 1.68 \cdot 10^{59} \text{ days} \approx 4.59 \cdot 10^{56} \text{ years}$$<p>That’s another huge number of years.<h2 id=seed-phrases-and-passwords>Seed Phrases and Passwords</h2><p>You can also use a seed phrase as a password. The bonus point is that you don’t need to use the last word as a checksum, so you get 11 bits of Entropy free, compared to a Bitcoin seed phrase.<p>Remember the 7-words badboy seed phrase we generated earlier? <code>brave sadness grocery churn wet mammal tube</code>.<p>It has $66$ bits of Entropy. This would take, assuming <a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>“that your adversary is capable of a trillion guesses per second”</a>:<p>$$\frac{2^{77}}{10^{12}} \approx 1.51 \cdot 10^{11} \text{ seconds} \approx 1.75 \cdot 10^{6} \text{ days} \approx 4.79 \cdot 10^{3} \text{ years}$$<p>That’s why tons of people use seed phrases as passwords. Even if you know the dictionary set and the length of the password, i.e. the number of words in the seed phrase, it would take a lot of years to crack it.<h2 id=conclusion>Conclusion</h2><p>Entropy is a measure of the amount of disorder in a system. In the context of passwords, it’s a measure of how unpredictable a password is. The higher the Entropy, the harder it is to guess the password.<p>Bitcoin seed phrases are a great way to store and recover private keys. They are easy to remember and have a high amount of Entropy. You can even use a seed phrase as a password.<p>Even it your attacker is capable of a trillion guesses per second, like the <a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>NSA</a>, it would take them a lot of years to crack even a 7-word seed phrase.<p>If you want to generate a seed phrase, you can use <a href=https://keepassxc.org/>KeePassXC</a>, which is a great open-source <strong><em>offline</em></strong> password manager that supports seed phrases.<blockquote><p><a href="https://storopoli.com/blog/mnemonic/$block.attrs('info')"></a> Technically, KeePassXC uses the <a href=https://www.eff.org/files/2016/07/18/eff_large_wordlist.txt>EFF wordlist</a>, which has 7,776 words, so each word gives you $\log_2(7776) \approx 12.9$ bits of Entropy. They were created to be easy to use with 6-sided dice.</blockquote></section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://storopoli.com/blog/mnemonic/#entropy>Entropy</a> <ul><li><a href=https://storopoli.com/blog/mnemonic/#how-the-hell-physics-came-to-passwords>How the hell Physics came to Passwords?</a><li><a href=https://storopoli.com/blog/mnemonic/#entropy-and-passwords>Entropy and Passwords</a></ul><li><a href=https://storopoli.com/blog/mnemonic/#bitcoin-seed-phrases>Bitcoin Seed Phrases</a><li><a href=https://storopoli.com/blog/mnemonic/#seed-phrases-and-passwords>Seed Phrases and Passwords</a><li><a href=https://storopoli.com/blog/mnemonic/#conclusion>Conclusion</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://storopoli.com/katex.min.css rel=stylesheet><script defer src=https://storopoli.com/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://storopoli.com/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://storopoli.com/atom.xml> <img alt=feed loading=lazy src=https://storopoli.com/social_icons/rss.svg title=feed> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=mailto:jose@storopoli.com> <img alt=email loading=lazy src=https://storopoli.com/social_icons/email.svg title=email> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://storopoli.com/publickey.txt> <img alt=pgp loading=lazy src=https://storopoli.com/social_icons/key.svg title=pgp> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/storopoli/> <img alt=github loading=lazy src=https://storopoli.com/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://matrix.to/#/@jose:storopoli.com> <img alt=matrix loading=lazy src=https://storopoli.com/social_icons/matrix.svg title=matrix> </a><li><a class="nav-links no-hover-padding social" href="https://simplex.chat/contact#/?v=2-5&smp=smp%3A%2F%2FUkMFNAXLXeAAe0beCa4w6X_zp18PwxSaSjY17BKUGXQ%3D%40smp12.simplex.im%2FUXrwU_eqdgeHQ6HYehFs0s8VRHOr3k47%23%2F%3Fv%3D1-2%26dh%3DMCowBQYDK2VuAyEApVAYxmE0bpIIiPftNjehy4qOoa14ubyEGzbRX_BlO0w%253D%26srv%3Die42b5weq7zdkghocs3mgxdjeuycheeqqmksntj57rmejagmg4eor5yd.onion" rel=" me"> <img alt=simplex loading=lazy src=https://storopoli.com/social_icons/simplex.svg title=simplex> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://orcid.org/0000-0002-0559-5176> <img alt=orcid loading=lazy src=https://storopoli.com/social_icons/orcid.svg title=orcid> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> • <a href=https://github.com/storopoli/storopoli.com> Site source </a></small></div></section></footer>